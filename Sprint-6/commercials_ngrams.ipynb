{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b7a450-fb07-466c-a861-6fd268a549aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db167c46-b220-488a-96c3-cb225cc08b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5955f9-9a44-4f2e-883d-0decb230997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in test_str:\n",
    "        if i == '(':\n",
    "            skip1c += 1\n",
    "        elif i == ')' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e8e9f6-d343-4877-ac9a-c91b51bff20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../Data/CorporaData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2f9727-baad-4965-98e0-4aa10b3fc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for folder in os.listdir(path):\n",
    "    if folder != '.DS_Store' and folder != '.ipynb_checkpoints':\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            if file != '.ipynb_checkpoints':\n",
    "                files.append(('../../Data/CommercialData/{}/{}_{}.txt'.format(\n",
    "                    file.split('.')[0], file.split('.')[0], file.split('.')[2]), os.path.join(path, os.path.join(folder, file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648098f3-4463-46ed-b9b0-e57e5c7cb1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of segments in CNBC_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2013.txt: 1.55 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2013.txt: 2.31 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2013.txt: 0.16 minutes\n",
      "\n",
      "Average number of segments in CNN_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2013.txt: 2.82 minutes\n",
      "\n",
      "Average number of segments in FBC_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2013.txt: 1.71 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2013.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2013.txt: 3.57 minutes\n",
      "\n",
      "Average number of segments in FBC_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2014.txt: 1.59 minutes\n",
      "\n",
      "Average number of segments in CNN_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2014.txt: 3.99 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2014.txt: 3.06 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2014.txt: 3.0 minutes\n",
      "\n",
      "Average number of segments in CNBC_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2014.txt: 2.57 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2014.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2014.txt: 4.46 minutes\n",
      "\n",
      "Average number of segments in CNBC_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2015.txt: 2.88 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2015.txt: 3.59 minutes\n",
      "\n",
      "Average number of segments in FBC_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2015.txt: 2.2 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2015.txt: 3.04 minutes\n",
      "\n",
      "Average number of segments in CNN_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2015.txt: 5.78 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2015.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2015.txt: 5.85 minutes\n",
      "\n",
      "Average number of segments in CNN_2012.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2012.txt: 5.25 minutes\n",
      "\n",
      "Average number of segments in FBC_2012.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2012.txt: 1.51 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2012.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2012.txt: 2.0 minutes\n",
      "\n",
      "Average number of segments in CNBC_2012.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2012.txt: 2.09 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2012.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2012.txt: 3.77 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2017.txt: 4.56 minutes\n",
      "\n",
      "Average number of segments in CNBC_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2017.txt: 2.27 minutes\n",
      "\n",
      "Average number of segments in FBC_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2017.txt: 1.59 minutes\n",
      "\n",
      "Average number of segments in CNN_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2017.txt: 3.57 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2017.txt: 3.5 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2017.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2017.txt: 3.46 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2019.txt: 3.72 minutes\n",
      "\n",
      "Average number of segments in CNN_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2019.txt: 4.17 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2019.txt: 2.61 minutes\n",
      "\n",
      "Average number of segments in FBC_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2019.txt: 1.97 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2019.txt: 2.49 minutes\n",
      "\n",
      "Average number of segments in CNBC_2019.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2019.txt: 1.47 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2021.txt: 0.8 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2021.txt: 0.84 minutes\n",
      "\n",
      "Average number of segments in CNBC_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2021.txt: 0.33 minutes\n",
      "\n",
      "Average number of segments in FBC_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2021.txt: 0.35 minutes\n",
      "\n",
      "Average number of segments in CNN_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2021.txt: 0.77 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2021.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2021.txt: 0.3 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2020.txt: 3.7 minutes\n",
      "\n",
      "Average number of segments in FBC_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2020.txt: 2.45 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2020.txt: 2.2 minutes\n",
      "\n",
      "Average number of segments in CNN_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2020.txt: 4.78 minutes\n",
      "\n",
      "Average number of segments in CNBC_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2020.txt: 2.73 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2020.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2020.txt: 4.85 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2018.txt: 4.77 minutes\n",
      "\n",
      "Average number of segments in CNBC_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2018.txt: 2.1 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2018.txt: 2.74 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2018.txt: 2.77 minutes\n",
      "\n",
      "Average number of segments in CNN_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2018.txt: 3.72 minutes\n",
      "\n",
      "Average number of segments in FBC_2018.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2018.txt: 2.05 minutes\n",
      "\n",
      "Average number of segments in FBC_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FBC_2016.txt: 1.73 minutes\n",
      "\n",
      "Average number of segments in Bloomberg_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in Bloomberg_2016.txt: 2.48 minutes\n",
      "\n",
      "Average number of segments in CNN_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNN_2016.txt: 3.71 minutes\n",
      "\n",
      "Average number of segments in CNBC_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in CNBC_2016.txt: 1.83 minutes\n",
      "\n",
      "Average number of segments in MSNBC_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in MSNBC_2016.txt: 2.51 minutes\n",
      "\n",
      "Average number of segments in FOXNEWS_2016.txt -- split by ♪: 0.0\n",
      "Time taken to find commercials in FOXNEWS_2016.txt: 3.18 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    # if os.path.exists(file[0]):\n",
    "    #     # print(file[0].split('/')[-1])\n",
    "    #     continue\n",
    "    # else:\n",
    "    start = time.time()\n",
    "    df = pd.read_csv(file[1])\n",
    "    df.columns = ['URL', 'Title', 'Text']\n",
    "    # Clean html, musical notes ♪, time stamps (check consecutive minutes)\n",
    "    df['Text'] = df['Text'].apply(lambda text: str(text))\n",
    "    df['Text'] = df['Text'].apply(lambda text: text.replace('[[TIME.START]] ', '('))\n",
    "    df['Text'] = df['Text'].apply(lambda text: text.replace(' [[TIME.END]]', ')'))\n",
    "    df['Text'] = df['Text'].apply(lambda text: str(text).split('TOPICS: TOPIC FREQUENCY ')[0])\n",
    "    df['Text'] = df['Text'].apply(lambda text: str(text)[str(text).find('[[TITLE.END]] ') + len('[[TITLE.END]] '):])\n",
    "    df['Text'] = df['Text'].apply(lambda text: re.sub('<[^>]+>', '', text))\n",
    "    df['Text'] = df['Text'].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "    df['Text'] = df['Text'].apply(lambda text: ' '.join([word for word in text.split() if word not in cachedStopWords]))\n",
    "    df['Text'] = df['Text'].apply(lambda text: a(text)) \n",
    "    # Pull all text data into a single string for analysis\n",
    "    combined_text = '. '.join([i for i in df['Text'][0:len(df)]])\n",
    "    # Find commercials by ngram frequency\n",
    "    tokens = get_ngrams(combined_text, 20)\n",
    "    freq = Counter(tokens)\n",
    "    # Go in and grab freq > 10\n",
    "    commercials = [i for i,m in freq.items() if m > 10]\n",
    "    print('Average number of segments in {} -- split by ♪: {}'.format(file[0].split('/')[4], round(len(combined_text.split('♪'))/len(df), 2)))\n",
    "    with open('{}'.format(file[0]), 'w') as f:\n",
    "        f.write('\\n'.join(commercials))\n",
    "    f.close()\n",
    "    end = time.time()\n",
    "    print('Time taken to find commercials in {}: {} minutes\\n'.format(file[0].split('/')[4], round((end-start)/60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a10df1-3a77-4ea1-9057-23be6b4174df",
   "metadata": {},
   "source": [
    "**Condense Commercials (reduce overhead when searching)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41860733-7559-4630-88dd-43ee9a5d0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_lines(lines):\n",
    "    comm_list = []\n",
    "    for index, line in enumerate(lines):\n",
    "        if line == '':\n",
    "            continue\n",
    "        running_line = line.split()\n",
    "        i, j = 0, 1\n",
    "        while index+j < len(lines) and running_line[1+i:] == lines[index+j].split()[:19]:\n",
    "            running_line.append(lines[index+j].split()[-1])\n",
    "            lines[index+j] = ''\n",
    "            i += 1\n",
    "            j += 1\n",
    "        comm_list.append(' '.join(running_line))\n",
    "        count = 0\n",
    "        for c in comm_list:\n",
    "            if lines[index] in c:\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            lines[index] = ''\n",
    "    return comm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b407532-222d-4445-be57-6d895a69e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file[0], 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) == 500:\n",
    "        ret = condense_lines(lines)\n",
    "        with open(file[0], 'w') as wf:\n",
    "            wf.write('\\n'.join(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47cc36-d812-45a5-b1cd-b88cbb95b633",
   "metadata": {},
   "source": [
    "**Check If Time Stamps are Consecutive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb44f9c-3691-4762-b635-1a1d4a0358cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for folder in os.listdir(path):\n",
    "    if folder != '.DS_Store' and folder != '.ipynb_checkpoints':\n",
    "        for file in os.listdir(os.path.join(path, folder)):\n",
    "            if file != '.ipynb_checkpoints':\n",
    "                files.append(os.path.join(path, os.path.join(folder, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed087d-ef0e-433d-95eb-45750c2cddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ['FOXNEWS', 'FBC', 'MSNBC', 'CNN', 'CNBC', 'Bloomberg']\n",
    "files_by_network = {}\n",
    "for n in networks:\n",
    "    files_by_network[n] = []\n",
    "    \n",
    "for file in files:\n",
    "    files_by_network[file.split('/')[4].split('.')[0]].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a7966-2cb4-4939-bfd7-4d6137d89081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in files_by_network.items():\n",
    "    for file in value:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = ['URL', 'Title', 'Text']\n",
    "        df = df.drop_duplicates(subset=['Title'])\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        df['Text'] = df['Text'].apply(lambda text: str(text))\n",
    "        df['Text'] = df['Text'].apply(lambda text: str(text).split('TOPICS: TOPIC FREQUENCY ')[0])\n",
    "        df['Text'] = df['Text'].apply(lambda text: str(text)[str(text).find('[[TITLE.END]] ') + len('[[TITLE.END]] '):])\n",
    "        df['Text'] = df['Text'].apply(lambda text: re.sub('<[^>]+>', '', text))\n",
    "        \n",
    "        list_of_times = {}\n",
    "        for i in range(len(df.index)):\n",
    "            list_of_times[df.loc[i, 'Title']] = []\n",
    "\n",
    "        for i in range(len(df.index)):\n",
    "            text = df.loc[i, 'Text'].split('[[TIME.START]]')\n",
    "            for segment in text:\n",
    "                if segment.split(' [[TIME.END]]')[0].split():\n",
    "                    list_of_times[df.loc[i, 'Title']].append(segment.split(' [[TIME.END]]')[0].split())\n",
    "                    \n",
    "        for k,v in list_of_times.items():\n",
    "            list_of_times[k] = [l[0] for l in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6686964-4955-46c9-87b6-65dec6d3985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shows_of_interest = []\n",
    "for key, value in list_of_times.items():\n",
    "    for index, elem in enumerate(value):\n",
    "        hour = elem.split(':')[0]\n",
    "        minute = elem.split(':')[1]\n",
    "        if index < len(value) - 1:\n",
    "            next_hour = value[index + 1].split(':')[0]\n",
    "            next_minute = value[index + 1].split(':')[1]\n",
    "            if int(hour) != 12:\n",
    "                if hour == next_hour:\n",
    "                    if int(next_minute) != int(minute) + 1:\n",
    "                        shows_of_interest.append((key, elem, value[index+1], 'minute skipped'))\n",
    "                elif int(next_hour) == int(hour)+1:\n",
    "                    if int(next_minute) != 0:\n",
    "                        shows_of_interest.append((key, elem, value[index+1], 'minute skipped'))\n",
    "                else:\n",
    "                    shows_of_interest.append((key, elem, value[index+1], 'hour skipped'))\n",
    "            else:\n",
    "                if hour == next_hour:\n",
    "                    if int(next_minute) != int(minute) + 1:\n",
    "                        shows_of_interest.append((key, elem, value[index+1], 'minute skipped'))\n",
    "                elif int(next_hour) == 1:\n",
    "                    if int(next_minute) != 0:\n",
    "                        shows_of_interest.append((key, elem, value[index+1], 'minute skipped'))\n",
    "                else:\n",
    "                    shows_of_interest.append((key, elem, value[index+1], 'hour skipped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f10390-d3bc-4567-abac-dd9cc3cc7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shows_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f03ff-6d88-4fea-820a-dad67487b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = df.loc[df['Title'] == 'Squawk Box : CNBC : October 30; 2013 6:00am-9:01am EDT'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e081a-e669-41db-b15b-160e63088884",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty = df.loc[ind, 'Text'].split('[[TIME.START]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ee769-e06a-4e6e-bcec-20beaddfe9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(faulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a7d91-d1fb-409c-8635-f65c915b80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faulty[170:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
