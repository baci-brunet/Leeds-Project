{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b911a41-f24a-44f0-ace0-4e99c96b23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a0eeb-4785-4fd5-befb-f0895daf7175",
   "metadata": {},
   "source": [
    "**Define Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbe3c3b-eb35-4a21-9257-ce451a9b77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_data = '../../Data/CorporaData'\n",
    "output_data = '../../OutputRaw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fff52-8278-438b-b4cd-ddcbed7dce56",
   "metadata": {},
   "source": [
    "**Iterate Through Directories and Gather File Tuples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f0812a-c4ab-4174-84a0-eb4eb842f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tuples = []\n",
    "for folder in os.listdir(corpora_data):\n",
    "    if folder != '.DS_Store':\n",
    "        for corpora_file in os.listdir(os.path.join(corpora_data, folder)):\n",
    "            if corpora_file != '.DS_Store' and corpora_file != '.ipynb_checkpoints':\n",
    "                tv_corpora = os.path.join(corpora_data + '/' + folder, corpora_file)\n",
    "                network = tv_corpora.split('/')[4]\n",
    "                search_key = '.'.join((network.split('.')[0], network.split('.')[2]))\n",
    "                for media_network in os.listdir(output_data):\n",
    "                    if media_network == search_key.split('.')[0]:\n",
    "                        for hits_file in os.listdir(os.path.join(output_data, media_network)):\n",
    "                            if hits_file.split('_')[1] == search_key.split('.')[1]:\n",
    "                                file_tuples.append((os.path.join(output_data, os.path.join(media_network, hits_file)), tv_corpora))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f38908-48be-4f6e-8967-089dcfae4a64",
   "metadata": {},
   "source": [
    "**Helper Function to Grab Context Sentences For Each Hit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9462445-044b-4849-ae29-3daddf86efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_context_window(tv_corpora, output):\n",
    "#     output['Context Window'] = None\n",
    "#     for i in range(len(output.index)):\n",
    "#         context = []\n",
    "#         list_of_indices = []\n",
    "#         firm = str(output.loc[i, 'Search Term'])\n",
    "#         raw_text = tv_corpora.loc[tv_corpora['Title'] == output.loc[i, 'Title']].index[0]\n",
    "#         for segment in tv_corpora.loc[raw_text, 'RawText_preprocessed'].split('[[TIME.START]]'):\n",
    "#             if output.loc[i, 'Time'] == segment.split('[[TIME.END]]')[0].strip():\n",
    "#                 list_of_indices\n",
    "#                 if '*' in firm:\n",
    "#                     search_term = r'\\b{}'.format(firm.strip('*'))\n",
    "#                 else:\n",
    "#                     search_term = r'\\b{}\\b'.format(firm)\n",
    "#                 if int(output.loc[i, 'Frequency']) > 1:\n",
    "#                     list_of_indices = [m.start() for m in re.finditer(search_term.lower(), segment.lower())]\n",
    "#                     for ind in list_of_indices:\n",
    "#                         if segment[ind-35:ind + len(firm) + 200] == '':\n",
    "#                             context_sentence = segment[ind:ind + len(firm) + 200]\n",
    "#                         else:\n",
    "#                             context_sentence = segment[ind-35:ind + len(firm) + 200]\n",
    "#                         # context_sentence = segment[ind-35:ind + len(firm) + 200]\n",
    "#                         context.append(context_sentence)\n",
    "#                     output.loc[i, 'Context Window'] = '\\n'.join(context)\n",
    "#                 else:\n",
    "#                     firm_ind = re.search(search_term.lower(), segment.lower()).start()\n",
    "#                     if segment[firm_ind-35:firm_ind + len(firm) + 200] == '':\n",
    "#                         output.loc[i, 'Context Window'] = segment[firm_ind:firm_ind + len(firm) + 200]\n",
    "#                     else:\n",
    "#                         output.loc[i, 'Context Window'] = segment[firm_ind-35:firm_ind + len(firm) + 200]\n",
    "                \n",
    "#             else:\n",
    "#                 continue\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0e68aeb-a28d-4d78-9254-49ce79880789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_window(tv_corpora, output):\n",
    "    output['Context Window'] = None\n",
    "    for i in range(len(output.index)):\n",
    "        context = []\n",
    "        list_of_indices = []\n",
    "        firm = str(output.loc[i, 'Search Term'])\n",
    "        raw_text_ind = tv_corpora.loc[tv_corpora['Title'] == output.loc[i, 'Title']].index[0]\n",
    "        text = tv_corpora.loc[raw_text_ind, 'RawText_preprocessed'].split('[[TIME.START]]')\n",
    "        for index,segment in enumerate(text):\n",
    "            if output.loc[i, 'Time'] == segment.split('[[TIME.END]]')[0].strip():\n",
    "                if index + 2 < len(text) and index - 2 >= 0:\n",
    "                    context = text[index - 2] + text[index - 1] + text[index] + text[index + 1] + text[index + 2]\n",
    "                    output.loc[i, 'Context Window'] = context\n",
    "                elif index + 1 < len(text) and index - 2 >= 0:\n",
    "                    context = text[index - 2] + text[index - 1] + text[index] + text[index + 1]\n",
    "                    output.loc[i, 'Context Window'] = context  \n",
    "                elif index - 1 >= 0 and index + 2 < len(text):\n",
    "                    context = text[index - 1] + text[index] + text[index + 1] + text[index + 2]\n",
    "                    output.loc[i, 'Context Window'] = context \n",
    "                elif index - 1 < 0 and index + 2 < len(text):\n",
    "                    context = text[index] + text[index + 1] + text[index + 2]\n",
    "                    output.loc[i, 'Context Window'] = context \n",
    "                elif index + 1 >= len(text) and index - 2 >= 0:\n",
    "                    context = text[index - 2] + text[index - 1] + text[index]\n",
    "                    output.loc[i, 'Context Window'] = context \n",
    "            else:\n",
    "                continue\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d76810-bd8d-4071-8867-cc72b3d7ac2d",
   "metadata": {},
   "source": [
    "**Add Context and Save as the New Output Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc5343-2bf0-4a8b-921b-c8b0c2405c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for files in file_tuples:\n",
    "    print('Reading File...')\n",
    "    output_file = pd.read_csv(files[0])\n",
    "    corpora_file = pd.read_csv(files[1])\n",
    "    corpora_file.columns = ['URL', 'Title', 'RawText']\n",
    "\n",
    "    # if 'Context Window' in output_file.columns:\n",
    "    #     count += 1\n",
    "    #     continue\n",
    "    # else:\n",
    "    print('Dropping Duplicates...')\n",
    "    if len(corpora_file) > len(output_file):\n",
    "        corpora_file.drop_duplicates(inplace=True)\n",
    "        corpora_file.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print('Preprocessing Raw Text...')\n",
    "    corpora_file['RawText_preprocessed'] = corpora_file['RawText'].apply(lambda text: str(text).split('TOPICS: TOPIC FREQUENCY ')[0])\n",
    "    corpora_file['RawText_preprocessed'] = corpora_file['RawText_preprocessed'].apply(lambda text: str(text)[str(text).find('[[TITLE.END]] ') + len('[[TITLE.END]] '):])       \n",
    "    corpora_file['RawText_preprocessed'] = corpora_file['RawText_preprocessed'].apply(lambda text: text.replace('â™ª', ''))\n",
    "    del corpora_file['RawText']\n",
    "\n",
    "    print('Pulling Context...')\n",
    "    start = time.time()\n",
    "    new_output_file = add_context_window(corpora_file, output_file)\n",
    "    new_output_file.to_csv(files[0], index=False)\n",
    "    end = time.time()\n",
    "    print('Time to add context words to {}: {} minutes'.format(files[0], (end - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8226d8c0-814f-48bc-8069-1a5007427ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0554d4-3be6-4596-961b-19e0a6001687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
